{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71885,"databundleVersionId":8143495,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":7884725,"sourceType":"datasetVersion","datasetId":4628331},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:13:54.189179Z","iopub.execute_input":"2025-01-24T07:13:54.189474Z","iopub.status.idle":"2025-01-24T07:13:59.192997Z","shell.execute_reply.started":"2025-01-24T07:13:54.189454Z","shell.execute_reply":"2025-01-24T07:13:59.191971Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nInstalling collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\n  Attempting uninstall: kornia-rs\n    Found existing installation: kornia_rs 0.1.8\n    Uninstalling kornia_rs-0.1.8:\n      Successfully uninstalled kornia_rs-0.1.8\n  Attempting uninstall: kornia\n    Found existing installation: kornia 0.7.4\n    Uninstalling kornia-0.7.4:\n      Successfully uninstalled kornia-0.7.4\nSuccessfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport os\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom time import time, sleep\nfrom fastprogress import progress_bar\nimport gc\nimport numpy as np\nimport h5py\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom typing import Any\nimport itertools\nimport pandas as pd\n\nimport cv2\nimport torch\nfrom torch import Tensor as T\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, AutoModel\n\nfrom lightglue import match_pair\nfrom lightglue import LightGlue, ALIKED\nfrom lightglue.utils import load_image, rbd\n\nimport pycolmap\n\nimport sys\nsys.path.append(\"/kaggle/input/colmap-db-import\")\n\nfrom database import *\nfrom h5_to_db import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T07:14:02.881081Z","iopub.execute_input":"2025-01-24T07:14:02.881388Z","iopub.status.idle":"2025-01-24T07:14:02.929787Z","shell.execute_reply.started":"2025-01-24T07:14:02.881363Z","shell.execute_reply":"2025-01-24T07:14:02.929081Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def arr_to_str(input_arr):\n    return ';'.join([str(x) for x in input_arr.reshape(-1)])\n\ndef load_torch_image(input_path, device=torch.device('cpu')):\n    input_img = K.io.load_image(input_path, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n\n    return input_img\n\n\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:59:36.970654Z","iopub.execute_input":"2025-01-24T08:59:36.971137Z","iopub.status.idle":"2025-01-24T08:59:36.977966Z","shell.execute_reply.started":"2025-01-24T08:59:36.971094Z","shell.execute_reply":"2025-01-24T08:59:36.977061Z"}},"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def get_global_descriptor(input_paths, device=torch.device('cpu')):\n    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = model.eval()\n    model = model.to(device)\n\n    global_desc_dinov2 = []\n\n    for idx, img_path in tqdm(enumerate(input_paths), total=len(input_paths)):\n        key = os.path.splitext(os.path.basename(img_path))[0]\n        torch_img = load_torch_image(img_path)\n\n        with torch.inference_mode():\n            inputs = processor(images=torch_img, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs)\n            dino_norm = F.normalize(outputs.last_hidden_state[:, 1:].max(dim=1)[0], dim=1, p=2)\n\n        global_desc_dinov2.append(dino_norm.detach().cpu())\n\n    global_desc_dinov2 = torch.cat(global_desc_dinov2, dim=0)\n\n    return global_desc_dinov2\n\ndef get_img_pairs(input_paths):\n    index_pairs = []\n\n    for i in range(len(input_paths)):\n        for j in range(i+1, len(input_paths)):\n            index_pairs.append((i, j))\n\n    return index_pairs\n\ndef get_img_pairs_shortlist(input_paths, sim_th=0.3, min_pairs=20, exhausive_if_less=20, device=torch.device('cpu')):\n    num_imgs = len(input_paths)\n    if num_imgs <= exhausive_if_less:\n        return get_img_pairs(input_paths)\n\n    descs = get_global_descriptor(input_paths, device=device)\n    descs_np = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n\n    mask = descs_np <= sim_th\n    total = 0\n\n    matching_list = []\n    already_there_set = []\n    num_arange = np.arange(num_imgs)\n\n    for idx in range(num_imgs - 1):\n        mask_idx = mask[idx]\n        to_match = num_arange[mask_idx]\n\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(descs_np[idx])[:min_pairs]\n\n        for i in to_match:\n            if idx == i:\n                continue\n            if descs_np[idx, i] < 1000:\n                matching_list.append(tuple(sorted((idx, i.item()))))\n                total += 1\n\n    matching_list = sorted(list(set(matching_list)))\n\n    return matching_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:59:38.511503Z","iopub.execute_input":"2025-01-24T08:59:38.511808Z","iopub.status.idle":"2025-01-24T08:59:38.522018Z","shell.execute_reply.started":"2025-01-24T08:59:38.511786Z","shell.execute_reply":"2025-01-24T08:59:38.521145Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def detect_aliked(input_paths, feature_path='.featureout', num_features=2048, resize_to=1024, device=torch.device('cpu')):\n    data_type = torch.float32\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.01, resize=resize_to)\n    extractor = extractor.eval()\n    extractor = extractor.to(device, data_type)\n\n    if not os.path.isdir(feature_path):\n        os.makedirs(feature_path)\n\n    with h5py.File(f'{feature_path}/keypoints.h5', mode='w') as feature_keypoints, h5py.File(f'{feature_path}/descriptors.h5', mode='w') as feature_descriptors:\n        for img_path in tqdm(input_paths):\n            img_filename = img_path.split('/')[-1]\n            key = img_filename\n\n            with torch.inference_mode():\n                input_img = load_torch_image(img_path, device=device).to(data_type)\n                img_feature = extractor.extract(input_img)\n                img_kpts = img_feature['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                img_descs = img_feature['descriptors'].reshape(len(img_kpts), -1).detach().cpu().numpy()\n\n                feature_keypoints[key] = img_kpts\n                feature_descriptors[key] = img_descs\n\n    return\n\ndef match_lightglue(input_paths, index_pairs, feature_path='.featureout', device=torch.device('cpu'), min_matches=15, verbose=False):\n    matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\":-1, \"depth_confidence\":-1, \"mp\":True if 'cuda' in str(device) else False})\n    matcher = matcher.eval()\n    matcher = matcher.to(device)\n\n    with h5py.File(f'{feature_path}/keypoints.h5', mode='r') as feature_keypoints, h5py.File(f'{feature_path}/descriptors.h5', mode='r') as feature_descriptors, h5py.File(f'{feature_path}/matches.h5', mode='w') as feature_matches:\n        for pair_index in tqdm(index_pairs):\n            idx_src, idx_dst = pair_index\n            filename_src, filename_dst = input_paths[idx_src], input_paths[idx_dst]\n            key_src, key_dst = filename_src.split('/')[-1], filename_dst.split('/')[-1]\n\n            keypoints_src = torch.from_numpy(feature_keypoints[key_src][...]).to(device)\n            keypoints_dst = torch.from_numpy(feature_keypoints[key_dst][...]).to(device)\n\n            descriptors_src = torch.from_numpy(feature_descriptors[key_src][...]).to(device)\n            descriptors_dst = torch.from_numpy(feature_descriptors[key_dst][...]).to(device)\n\n            with torch.inference_mode():\n                dists, idxs = matcher(descriptors_src, descriptors_dst, KF.laf_from_center_scale_ori(keypoints_src[None]), KF.laf_from_center_scale_ori(keypoints_dst[None]))\n\n            if len(idxs) == 0:\n                continue\n\n            num_matches = len(idxs)\n\n            if verbose:\n                print(f'{key_src}-{key_dst}: {num_matches} matches')\n\n            group = feature_matches.require_group(key_src)\n\n            if num_matches >= min_matches:\n                group.create_dataset(key_dst, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n\n    return\n\ndef import_into_colmap(img_dir, feature_dir='.featureout', db_path='colmap.db'):\n    db = COLMAPDatabase.connect(db_path)\n    db.create_tables()\n\n    is_single_cam = False\n    filename_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', is_single_cam)\n    add_matches(db, feature_dir, filename_to_id)\n\n    db.commit()\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:59:41.812154Z","iopub.execute_input":"2025-01-24T08:59:41.812447Z","iopub.status.idle":"2025-01-24T08:59:41.823706Z","shell.execute_reply.started":"2025-01-24T08:59:41.812426Z","shell.execute_reply":"2025-01-24T08:59:41.822884Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"input_src = '/kaggle/input/image-matching-challenge-2024/'\ndata_dict = {}\n\nwith open(f'{input_src}/sample_submission.csv', 'r') as submission_file:\n    for idx, val in enumerate(submission_file):\n        if (idx == 0):\n            print(val)\n\n        if val and idx > 0:\n            img_path, dataset, scene, _, _ = val.strip().split(',')\n\n            if dataset not in data_dict:\n                data_dict[dataset] = {}\n\n            if scene not in data_dict[dataset]:\n                data_dict[dataset][scene] = []\n\n            data_dict[dataset][scene].append(img_path)\n\nfor dataset in data_dict:\n    for scene in data_dict[dataset]:\n        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n\noutput_results = {}\ntimings = {\"shortlisting\":[], \"feature_detection\":[], \"feature_matching\":[], \"RANSAC\":[], \"Reconstruction\":[]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:59:44.937684Z","iopub.execute_input":"2025-01-24T08:59:44.937995Z","iopub.status.idle":"2025-01-24T08:59:44.946746Z","shell.execute_reply.started":"2025-01-24T08:59:44.937971Z","shell.execute_reply":"2025-01-24T08:59:44.945896Z"}},"outputs":[{"name":"stdout","text":"image_path,dataset,scene,rotation_matrix,translation_vector\n\nchurch / church -> 41 images\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def create_submission(output_results, data_dict):\n    with open(f'submission.csv', 'w') as result_file:\n        result_file.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n\n        for dataset in data_dict:\n            if dataset in output_results:\n                result = output_results[dataset]\n            else:\n                result = {}\n\n            for scene in data_dict[dataset]:\n                if scene in result:\n                    scene_result = result[scene]\n                else:\n                    scene_result = {\"R\":{}, \"t\":{}}\n\n                for image in data_dict[dataset][scene]:\n                    if image in scene_result:\n                        # print(image)\n                        R = scene_result[image]['R'].reshape(-1)\n                        T = scene_result[image]['t'].reshape(-1)\n                    else:\n                        R = np.eye(3).reshape(-1)\n                        T = np.zeros((3))\n\n                    result_file.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:59:47.408002Z","iopub.execute_input":"2025-01-24T08:59:47.408327Z","iopub.status.idle":"2025-01-24T08:59:47.413602Z","shell.execute_reply.started":"2025-01-24T08:59:47.408298Z","shell.execute_reply":"2025-01-24T08:59:47.412922Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"gc.collect()\n\ndatasets = []\n\nfor dataset in data_dict:\n    datasets.append(dataset)\n\nfor dataset in data_dict:\n    print(dataset)\n\n    if dataset not in output_results:\n        output_results[dataset] = {}\n\n    for scene in data_dict[dataset]:\n        print(scene)\n\n        img_dir = os.path.join(input_src, '/'.join(data_dict[dataset][scene][0].split('/')[:-1]))\n\n        try:\n            output_results[dataset][scene] = {}\n\n            input_paths = [os.path.join(input_src, x) for x in data_dict[dataset][scene]]\n            feature_path = f'featureout/{dataset}/{scene}'\n            os.makedirs(feature_path, exist_ok=True)\n\n            t = time()\n            index_pairs = get_img_pairs_shortlist(input_paths, device=device)\n            t = time() - t\n            timings['shortlisting'].append(t)\n\n            print(f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n            gc.collect()\n\n            t = time()\n            detect_aliked(input_paths, feature_path, device=device)\n            t = time() - t\n            timings['feature_detection'].append(t)\n\n            print(f'Features detected in {t:.4f} sec')\n            gc.collect()\n\n            t = time()\n            match_lightglue(input_paths, index_pairs, feature_path, device=device)\n            t = time() - t\n            timings['feature_matching'].append(t)\n\n            print(f'Features matched in {t:.4f} sec')\n            gc.collect()\n\n            db_path = f'{feature_path}/colmap.db'\n            if os.path.isfile(db_path):\n                os.remove(db_path)\n\n            sleep(1)\n\n            import_into_colmap(img_dir, feature_path, db_path)\n\n            t = time()\n            pycolmap.match_exhaustive(db_path)\n            t = time() - t\n            timings['RANSAC'].append(t)\n\n            print(f'RANSAC in {t:.4f} sec')\n\n            t = time()\n            recon_options = pycolmap.IncrementalPipelineOptions()\n            recon_options.min_model_size = 3\n            recon_options.max_num_models = 2\n\n            output_path = f'{feature_path}/colmap_rec_aliked'\n            os.makedirs(output_path, exist_ok=True)\n\n            recon_results = pycolmap.incremental_mapping(database_path=db_path, image_path=img_dir, output_path=output_path, options=recon_options)\n\n            sleep(1)\n\n            print(recon_results)\n            clear_output(wait=False)\n            t = time() - t\n            timings['Reconstruction'].append(t)\n\n            print(f'Reconstruction done in {t:.4f} sec')\n\n            imgs_registered = 0\n            best_idx = None\n\n            if isinstance(recon_results, dict):\n                for idx, rec in recon_results.items():\n                    try:\n                        if len(rec.images) > imgs_registered:\n                            imgs_registered = len(rec.images)\n                            best_idx = idx\n                    except:\n                        continue\n\n            if best_idx is not None:\n                for key, img in recon_results[best_idx].images.items():\n                    key_img = f'test/{scene}/images/{img.name}'\n\n                    output_results[dataset][scene][key_img] = {}\n                    output_results[dataset][scene][key_img][\"R\"] = deepcopy(img.cam_from_world.rotation.matrix())\n                    output_results[dataset][scene][key_img][\"t\"] = deepcopy(np.array(img.cam_from_world.translation))\n\n            create_submission(output_results, data_dict)\n            gc.collect()\n        except Exception as e:\n            print(e)\n            pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T08:59:49.856918Z","iopub.execute_input":"2025-01-24T08:59:49.857230Z","iopub.status.idle":"2025-01-24T09:02:30.088237Z","shell.execute_reply.started":"2025-01-24T08:59:49.857204Z","shell.execute_reply":"2025-01-24T09:02:30.087533Z"}},"outputs":[{"name":"stdout","text":"Reconstruction done in 84.5267 sec\n","output_type":"stream"}],"execution_count":31}]}